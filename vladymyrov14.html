<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title><span>Linear-time training of nonlinear low-dimensional embeddings</span> | AISTATS 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="{Linear-time training of nonlinear low-dimensional embeddings}">

  <meta name="citation_author" content="Vladymyrov, Max">

  <meta name="citation_author" content="Carreira-Perpinan, Miguel">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="968">
<meta name="citation_lastpage" content="977">
<meta name="citation_pdf_url" content="vladymyrov14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1><span>Linear-time training of nonlinear low-dimensional embeddings</span></h1>

	<div id="authors">
	
		Max Vladymyrov,
	
		Miguel Carreira-Perpinan
	</div>;
	<div id="info">
		JMLR W&amp;CP 33 
		
		: 
		968–977, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an <span class="math">\(N\)</span>-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the <span class="math">\(\mathcal{O}(N \log N)\)</span> Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours’ runtime.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="vladymyrov14.pdf">Download PDF</a></li>
			
			<li><a href="vladymyrov14-supp.tgz">Supplementary (TGZ)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
