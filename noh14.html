<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title><span>Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence</span> | AISTATS 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="{Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence}">

  <meta name="citation_author" content="Noh, Yung-Kyun">

  <meta name="citation_author" content="Sugiyama, Masashi">

  <meta name="citation_author" content="Liu, Song">

  <meta name="citation_author" content="du Plessis, Marthinus C.">

  <meta name="citation_author" content="Park, Frank Chongwoo">

  <meta name="citation_author" content="Lee, Daniel D.">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="669">
<meta name="citation_lastpage" content="677">
<meta name="citation_pdf_url" content="noh14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1><span>Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence</span></h1>

	<div id="authors">
	
		Yung-Kyun Noh,
	
		Masashi Sugiyama,
	
		Song Liu,
	
		Marthinus C. du Plessis,
	
		Frank Chongwoo Park,
	
		Daniel D. Lee
	</div>;
	<div id="info">
		JMLR W&amp;CP 33 
		
		: 
		669â€“677, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Asymptotically unbiased nearest-neighbor estimators for K-L divergence have recently been proposed and demonstrated in a number of applications. With small sample sizes, however, these nonparametric methods typically suffer from high estimation bias due to the non-local statistics of empirical nearest-neighbor information. In this paper, we show that this non-local bias can be mitigated by changing the distance metric, and we propose a method for learning an optimal Mahalanobis-type metric based on global information provided by approximate parametric models of the underlying densities. In both simulations and experiments, we demonstrate that this interplay between parametric models and nonparametric estimation methods significantly improves the accuracy of the nearest-neighbor K-L divergence estimator.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="noh14.pdf">Download PDF</a></li>
			
			<li><a href="noh14-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
