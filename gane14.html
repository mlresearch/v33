<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title><span>Learning with Maximum A-Posteriori Perturbation Models</span> | AISTATS 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="{Learning with Maximum A-Posteriori Perturbation Models}">

  <meta name="citation_author" content="Gane, Andreea">

  <meta name="citation_author" content="Hazan, Tamir">

  <meta name="citation_author" content="Jaakkola, Tommi">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="247">
<meta name="citation_lastpage" content="256">
<meta name="citation_pdf_url" content="gane14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1><span>Learning with Maximum A-Posteriori Perturbation Models</span></h1>

	<div id="authors">
	
		Andreea Gane,
	
		Tamir Hazan,
	
		Tommi Jaakkola
	</div>;
	<div id="info">
		JMLR W&amp;CP 33 
		
		: 
		247–256, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Perturbation models are families of distributions induced from perturbations. They combine randomization of the parameters with maximization to draw unbiased samples. Unlike Gibbs’ distributions, a perturbation model defined on the basis of low order statistics still gives rise to high order dependencies. In this paper, we analyze, extend and seek to estimate such dependencies from data. In particular, we shift the modelling focus from the parameters of the Gibbs’ distribution used as a base model to the space of perturbations. We estimate dependent perturbations over the parameters using a hard-EM approach, cast in the form of inverse convex programs. Each inverse program confines the randomization to the parameter polytope responsible for generating the observed answer. We illustrate the method on several computer vision problems.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="gane14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
