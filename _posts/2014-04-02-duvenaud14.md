---
title: Avoiding pathologies in very deep networks
abstract: Choosing appropriate architectures and regularization strategies of deep
  networks is crucial to good predictive performance.  To shed light on this problem,
  we analyze the analogous problem of constructing useful priors on compositions of
  functions.  Specifically, we study the deep Gaussian process, a type of infinitely-wide,
  deep neural network.  We show that in standard architectures, the representational
  capacity of the network tends to capture fewer degrees of freedom as the number
  of layers increases, retaining only a single degree of freedom in the limit.  We
  propose an alternate network architecture which does not suffer from this pathology.  We
  also examine deep covariance functions, obtained by composing infinitely many feature
  transforms.  Lastly, we characterize the class of models obtained by performing
  dropout on Gaussian processes.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: duvenaud14
month: 0
tex_title: "{Avoiding pathologies in very deep networks}"
firstpage: 202
lastpage: 210
page: 202-210
sections: 
author:
- given: David
  family: Duvenaud
- given: Oren
  family: Rippel
- given: Ryan
  family: Adams
- given: Zoubin
  family: Ghahramani
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/duvenaud14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
