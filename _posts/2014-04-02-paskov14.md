---
supplementary: Supplementary:paskov14-supp.pdf
title: "{An Efficient Algorithm for Large Scale Compressive Feature Learning}"
abstract: This paper focuses on large-scale unsupervised feature selection from text.
  We expand upon the recently proposed Compressive Feature Learning (CFL) framework,
  a method that uses dictionary-based compression to select a K-gram representation
  for a document corpus. We show that CFL is NP-Complete and provide a novel and efficient
  approximation algorithm based on a homotopy that transforms a convex relaxation
  of CFL into the original problem. Our algorithm allows CFL to scale to corpuses
  comprised of millions of documents because each step is linear in the corpus length
  and highly parallelizable. We use it to extract features from the BeerAdvocate dataset,
  a corpus of over 1.5 million beer reviews spanning 10 years. CFL uses two orders
  of magnitude fewer features than the full trigram space. It beats a standard unigram
  model in a number of prediction tasks and achieves nearly twice the accuracy on
  an author identification task.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: paskov14
month: 0
firstpage: 760
lastpage: 768
page: 760-768
sections: 
author:
- given: Hristo
  family: Paskov
- given: John
  family: Mitchell
- given: Trevor
  family: Hastie
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/paskov14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
