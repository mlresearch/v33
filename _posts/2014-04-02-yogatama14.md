---
title: "{Efficient Transfer Learning Method for Automatic Hyperparameter Tuning}"
abstract: We propose a fast and effective algorithm for automatic hyperparameter tuning
  that can generalize across datasets. Our method is an instance of sequential model-based
  optimization (SMBO) that transfers information by constructing a common response
  surface for all datasets, similar to Bardenet et al. (2013). The time complexity
  of reconstructing the response surface at every SMBO iteration in our method is
  linear in the number of trials (significantly less than previous work with comparable
  performance), allowing the method to realistically scale to many more datasets.
  Specifically, we use deviations from the per-dataset mean as the response values.
  We empirically show the superiority of our method on a large number of synthetic
  and real-world datasets for tuning hyperparameters of logistic regression and ensembles
  of classifiers.
layout: inproceedings
id: yogatama14
month: 0
firstpage: 1077
lastpage: 1085
page: 1077-1085
sections: 
author:
- given: Dani
  family: Yogatama
- given: Gideon
  family: Mann
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/yogatama14/yogatama14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
