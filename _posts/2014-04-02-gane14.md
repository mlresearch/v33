---
title: Learning with Maximum A-Posteriori Perturbation Models
abstract: Perturbation models are families of distributions induced from perturbations.
  They combine randomization of the parameters with maximization to draw unbiased
  samples. Unlike Gibbs’ distributions, a perturbation model defined on the basis
  of low order statistics still gives rise to high order dependencies. In this paper,
  we analyze, extend and seek to estimate such dependencies from data. In particular,
  we shift the modelling focus from the parameters of the Gibbs’ distribution used
  as a base model to the space of perturbations. We estimate dependent perturbations
  over the parameters using a hard-EM approach, cast in the form of inverse convex
  programs. Each inverse program confines the randomization to the parameter polytope
  responsible for generating the observed answer. We illustrate the method on several
  computer vision problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gane14
month: 0
tex_title: "{Learning with Maximum A-Posteriori Perturbation Models}"
firstpage: 247
lastpage: 256
page: 247-256
sections: 
author:
- given: Andreea
  family: Gane
- given: Tamir
  family: Hazan
- given: Tommi
  family: Jaakkola
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/gane14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
