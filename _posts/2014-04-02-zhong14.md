---
title: "{Accelerated Stochastic Gradient Method for Composite Regularization}"
abstract: Regularized risk minimization often involves nonsmooth optimization. This
  can be particularly challenging when the regularizer is a sum of simpler regularizers,
  as in the overlapping group lasso. Very recently, this is alleviated by using the
  proximal average, in which an implicitly nonsmooth function is employed to approximate
  the composite regularizer. In this paper, we propose a novel extension with accelerated
  gradient method for stochastic optimization. On both general convex and strongly
  convex problems, the resultant approximation errors reduce at a faster rate than
  methods based on stochastic smoothing and ADMM. This is also verified experimentally
  on a number of synthetic and real-world data sets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhong14
month: 0
firstpage: 1086
lastpage: 1094
page: 1086-1094
sections: 
author:
- given: Wenliang
  family: Zhong
- given: James
  family: Kwok
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/zhong14/zhong14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
