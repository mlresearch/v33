---
supplementary: Supplementary:noh14-supp.pdf
title: "{Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler
  Divergence}"
abstract: Asymptotically unbiased nearest-neighbor estimators for K-L divergence have
  recently been proposed and demonstrated in a number of applications.  With small
  sample sizes, however, these nonparametric methods typically suffer from high estimation
  bias due to the non-local statistics of empirical nearest-neighbor information.  In
  this paper, we show that this non-local bias can be mitigated by changing the distance
  metric, and we propose a method for learning an optimal Mahalanobis-type metric
  based on global information provided by approximate parametric models of the underlying
  densities. In both simulations and experiments, we demonstrate that this interplay
  between parametric models and nonparametric estimation methods significantly improves
  the accuracy of the nearest-neighbor K-L divergence estimator.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: noh14
month: 0
firstpage: 669
lastpage: 677
page: 669-677
sections: 
author:
- given: Yung-Kyun
  family: Noh
- given: Masashi
  family: Sugiyama
- given: Song
  family: Liu
- given: Marthinus C.
  family: Plessis
- given: Frank Chongwoo
  family: Park
- given: Daniel D.
  family: Lee
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/noh14/noh14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
