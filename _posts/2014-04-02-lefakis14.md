---
title: Jointly Informative Feature Selection
abstract: We propose several novel criteria for the selection of groups of jointly
  informative continuous features in the context of classification. Our approach is
  based on combining a Gaussian modeling of the feature responses, with derived upper
  bounds on their mutual information with the class label and their joint entropy.    We
  further propose specific algorithmic implementations of these criteria which reduce
  the computational complexity of the algorithms  by up to two-orders of magnitude,
  making these strategies tractable in practice.    Experiments on multiple computer-vision
  data-bases, and using several types of classifiers, show that this class of methods
  outperforms state-of-the-art baselines, both in terms of speed and classification
  accuracy.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lefakis14
month: 0
tex_title: "{Jointly Informative Feature Selection}"
firstpage: 567
lastpage: 575
page: 567-575
order: 567
cycles: false
author:
- given: Leonidas
  family: Lefakis
- given: Francois
  family: Fleuret
date: 2014-04-02
address: Reykjavik, Iceland
publisher: PMLR
container-title: Proceedings of the Seventeenth International Conference on Artificial
  Intelligence and Statistics
volume: '33'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 4
  - 2
pdf: http://proceedings.mlr.press/v33/lefakis14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
